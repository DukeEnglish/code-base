{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lijunyi/anaconda3/envs/tensorflow/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# 导入相关的函数包\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cross_validation import train_test_split  \n",
    "from tokenize import Ignore\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "朴素贝叶斯的介绍：\n",
    "https://www.cnblogs.com/pinard/p/6069267.html\n",
    "\n",
    "我们知道，在进行模型的总结的时候，我们应该把模型的定义和数据的读取分别定义两个类，这样可以使得代码更加的整洁容易读\n",
    "\n",
    "说明：\n",
    "\n",
    "训练集(Training set)\n",
    "作用是用来拟合模型，通过设置分类器的参数，训练分类模型。后续结合验证集作用时，会选出同一参数的不同取值，拟合出多个分类器。\n",
    "\n",
    "\n",
    "\n",
    "验证集(Validation set)\n",
    "\n",
    "作用是当通过训练集训练出多个模型后，为了能找出效果最佳的模型，使用各个模型对验证集数据进行预测，并记录模型准确率。选出效果最佳的模型所对应的参数，即用来调整模型参数。如svn中的参数c和核函数等。\n",
    "\n",
    "\n",
    "\n",
    "测试集(Test set)\n",
    "\n",
    "通过训练集和验证集得出最优模型后，使用测试集进行模型预测。用来衡量该最优模型的性能和分类能力。即可以把测试集当做从来不存在的数据集，当已经确定模型参数后，使用测试集进行模型性能评价。\n",
    "\n",
    "\n",
    "我们在下面的类的定义中直接将所有的数据都读取，然后对数据进行\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 朴素贝叶斯分类器的类定义\n",
    "# 关于朴素贝叶斯分类起的使用，我们首先要进行文本表示，变成向量之后在进行进一步的处理\n",
    "# 关于贝叶斯分类的ppt以及相关的介绍和知识\n",
    "class NBclassifier():\n",
    "    def __init__(self, is_fit = True, clf_path=None,vec_path=None):\n",
    "        '''\n",
    "        参数：\n",
    "        is_fit: 是否从数据中学习类别的先验概率\n",
    "        clf_path：分类器的路径\n",
    "        vec_path：向量的路径\n",
    "        创建对象时完成的初始化工作，判断分类器与vector路径是否为空，\n",
    "        若为空则创建新的分类器与vector，否则直接加载已经持久化的分类器与vector。\n",
    "        '''\n",
    "        print('是否从数据中学习类别的先验概率', is_fit)\n",
    "        if (clf_path==None or vec_path==None):\n",
    "            self.clf=MultinomialNB(fit_prior = is_fit)\n",
    "            self.vec=TfidfVectorizer()\n",
    "        else:\n",
    "            self.clf=joblib.load(clf_path)\n",
    "            self.vec=joblib.load(vec_path)\n",
    "\n",
    "    # 保存模型 为了未来使用训练好的模型\n",
    "    def saveModel(self,clf_path=\"clf.m\",vec_path=\"vec.m\"):\n",
    "        joblib.dump(self.clf,clf_path)\n",
    "        joblib.dump(self.vec,vec_path)\n",
    "\n",
    "    #训练分类器\n",
    "    def trainNB(self,dataList,labelList):\n",
    "    #训练模型首先需要将分好词的文本进行向量化，这里使用的TFIDF进行向量化\n",
    "        try:\n",
    "            self.clf.fit(self.vec.fit_transform(dataList),labelList) # 使用tfidf向量化后的词进行训练\n",
    "            print(self.clf.class_log_prior_ ) \n",
    "        except:\n",
    "            print('wtf')\n",
    "        self.saveModel()\n",
    "\n",
    "    #预测数据 - 只有概率最高的那个结果\n",
    "    def predictNB(self,dataList):\n",
    "        data = self.vec.transform(dataList)\n",
    "#         print(dataList)\n",
    "        predictList=self.clf.predict(data) # 直接返回预测结果 \n",
    "        return predictList\n",
    "    \n",
    "    def preprob(self, data): # 返回每个类相应的概率\n",
    "        data = self.vec.transform(data)\n",
    "        return self.clf.predict_proba(data)\n",
    "    \n",
    "    \n",
    "    def prelog_prob(self, data): # 返回log后的概率\n",
    "        data = self.vec.transform(data)\n",
    "        return self.clf.predict_log_proba(data)\n",
    "    \n",
    "    #计算准确率\n",
    "    def calAccuracy(self,predictList,labelList,):\n",
    "        rightCount=0\n",
    "        print('测试数据集个数为',len(labelList))\n",
    "        for i in range(len(labelList)):\n",
    "            l = set(labelList[i].strip().split('\\t'))\n",
    "            if predictList[i] in l:\n",
    "                rightCount+=1\n",
    "        print ('准确率为：%s' %(rightCount/float(len(labelList)))) \n",
    "\n",
    "    def cal_precision(self,predictList,labelList):\n",
    "        rightCount=0\n",
    "        precount=0\n",
    "        label_count=0\n",
    "        for i in range(len(labelList)):\n",
    "            l = set(labelList[i].strip().split(','))\n",
    "            for p in predictList[i]:\n",
    "                if p[0] in l:\n",
    "                    rightCount+=1\n",
    "                precount+=1\n",
    "            label_count+=len(l)\n",
    "            p=(rightCount/float(precount))\n",
    "            r=(rightCount/float(label_count))\n",
    "        print ('precision：%s' %(rightCount/float(precount)))   \n",
    "        print ('recall：%s' %(rightCount/float(label_count))) \n",
    "        print('f1-measure: %s' %((2*p*r)/(p+r)))\n",
    "    \n",
    "    def cal_precision_topk(self, predictList, labelList):\n",
    "        rightCount=0\n",
    "        precount=0\n",
    "        label_count=0\n",
    "        for i in range(len(labelList)):\n",
    "            l = set(labelList[i].strip().split(','))\n",
    "            for p in predictList[i]:# for each case\n",
    "                if p[1] in l:\n",
    "                    rightCount+=1\n",
    "                precount+=1\n",
    "            label_count+=len(l)\n",
    "            \n",
    "        print ('precision：%s' %(rightCount/float(precount)))   \n",
    "        print ('recall：%s' %(rightCount/float(label_count))) \n",
    "            \n",
    "    # 本函数可以用来输出所有大于阈值的类别标签\n",
    "    def get_nbest(self, probs, labels, thread):\n",
    "        '''\n",
    "        probs是每个case针对每个类别的概率 n*c的数组\n",
    "        labels是类别的list，大小为训练样本中不同的class数目。使用np.unique(labelList)获得，在本问题中是这样的。\n",
    "        thread是阈值，用来判断多大的概率应该被输出\n",
    "        '''\n",
    "        output=[] # 将输出保存在output中\n",
    "        for t in [zip(i,np.unique(labels)) for i in probs]:\n",
    "            output.append(list(t)) # 将每个元组保存在output中，元组构成为 '概率','标签'\n",
    "\n",
    "    # output示例：\n",
    "    # 大小为：n*c*2\n",
    "    # 格式为：\n",
    "    # [[()]]\n",
    "    # 最外层数组为case的index\n",
    "    # 往里一层是针对具体的case的类别index\n",
    "    # 最里层是元组，元组构成为。元组构成为 '概率','标签'\n",
    "\n",
    "        # 预测归一化概率大于thread的结果\n",
    "        \n",
    "        results=[]\n",
    "        results_top3=[]\n",
    "        for i in output: \n",
    "            t=np.mean(list(zip(*i))[0])\n",
    "            result=[]\n",
    "            result_top3=[]\n",
    "            for p in range(len(i)):\n",
    "                s=[]\n",
    "                if i[p][0]>t:\n",
    "                    s.append(i[p][1])\n",
    "                    s.append(i[p][0])\n",
    "                    result.append(s)\n",
    "            results_top3.append(sorted(i,reverse=True)[:3])\n",
    "            results.append(result)\n",
    "        return results, results_top3 # 返回的是满足阈值要求的标签，概率对\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_helper():\n",
    "    '''\n",
    "    这个类定义了读取数据的方法，并且内置了jieba分词，并且可以根据中文或者英文进行区分。\n",
    "    参数介绍：\n",
    "    datapath:数据路径\n",
    "    train：读取的数据是否为训练数据，默认为“是”\n",
    "    Chinese：是否为中文，默认为“是”\n",
    "    '''\n",
    "    def __init__(self, datapath, train = True, Chinese = True, read_fr_file):\n",
    "        if read_fr_file:\n",
    "            if train:\n",
    "                dataList=[]\n",
    "                labelList=[]\n",
    "                for line in open(dataPath,encoding='utf-8').readlines():\n",
    "                    lineArray=line.split('\\t')\n",
    "                    labelList.append(lineArray[-2])\n",
    "                    dataList.append(lineArray[1]+lineArray[2]) \n",
    "                    thread+=1\n",
    "                    if thread==r: # 限制加载的训练数据个数\n",
    "                        break\n",
    "                print('长度是{0}'.format(len(dataList)))\n",
    "                if Chinese:\n",
    "                    return self.jiebaSplit(dataList), labelList\n",
    "                else:\n",
    "                    return dataList, labelList\n",
    "            else:\n",
    "                dataList=[]\n",
    "                labelList=[]\n",
    "                for line in open(dataPath,encoding='utf-8').readlines():\n",
    "                    lineArray=line.split('\\t')\n",
    "                    labelList.append(lineArray[-1])\n",
    "                    dataList.append(lineArray[1]+lineArray[2])\n",
    "                    thread+=1\n",
    "                    if thread==r:\n",
    "                        break\n",
    "                print('长度是{0}'.format(len(dataList)))\n",
    "                if Chinese:\n",
    "                    return self.jiebaSplit(dataList), labelList\n",
    "                else:\n",
    "                    return dataList, labelList\n",
    "        else:\n",
    "            if train:\n",
    "                dataList=[]\n",
    "                labelList=[]\n",
    "                for line in open(dataPath,encoding='utf-8').readlines():\n",
    "                    lineArray=line.split('\\t')\n",
    "                    labelList.append(lineArray[-2])\n",
    "                    dataList.append(lineArray[1]+lineArray[2]) \n",
    "                    thread+=1\n",
    "                    if thread==r: # 限制加载的训练数据个数\n",
    "                        break\n",
    "                print('长度是{0}'.format(len(dataList)))\n",
    "                if Chinese:\n",
    "                    return self.jiebaSplit(dataList), labelList\n",
    "                else:\n",
    "                    return dataList, labelList\n",
    "            else:\n",
    "                dataList=[]\n",
    "                labelList=[]\n",
    "                for line in open(dataPath,encoding='utf-8').readlines():\n",
    "                    lineArray=line.split('\\t')\n",
    "                    labelList.append(lineArray[-1])\n",
    "                    dataList.append(lineArray[1]+lineArray[2])\n",
    "                    thread+=1\n",
    "                    if thread==r:\n",
    "                        break\n",
    "                print('长度是{0}'.format(len(dataList)))\n",
    "                if Chinese:\n",
    "                    return self.jiebaSplit(dataList), labelList\n",
    "                else:\n",
    "                    return dataList, labelList\n",
    "            \n",
    "                \n",
    "    # 使用结巴进行分词\n",
    "    def jiebaSplit(self, inputData):\n",
    "        '''\n",
    "        简单描述一下jieba分词。\n",
    "        '''\n",
    "        stopWords = open(\"../../NLP/中文停用词.txt\").read().split(\"\\n\") # 加载停用词，网上一把一把\n",
    "        results=[] # 存放结果\n",
    "        for Data in inputData: \n",
    "            Data = \"\".join(re.findall(u'[\\u4e00-\\u9fa5]+', Data)) # 首先将数据中所有的中文提取出来\n",
    "            wordList = \"/\".join(jieba.cut(Data)) # 使用jieba进行切词，并且把每个词语使用/分隔开\n",
    "            listOfTokens = wordList.split(\"/\") # \n",
    "            x= [tok for tok in listOfTokens if (tok not in stopWords and len(tok) >= 2)] # 注意python3 写成一行的if 语句需要用括号\n",
    "            x=list(set(x)) # 使用词集\n",
    "            '''\n",
    "            词集和词袋的区别：\n",
    "            词集里面只考虑每个词是否出现\n",
    "            词袋同时也会考虑每个词出现的次数\n",
    "            \n",
    "            '''\n",
    "            results.append(' '.join(x))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jiebaSplit(self, inputData):\n",
    "        '''\n",
    "        简单描述一下jieba分词。\n",
    "        '''\n",
    "        stopWords = open(\"../../NLP/中文停用词.txt\").read().split(\"\\n\") # 加载停用词，网上一把一把\n",
    "        results=[] # 存放结果\n",
    "        for Data in inputData: \n",
    "            Data = \"\".join(re.findall(u'[\\u4e00-\\u9fa5]+', Data)) # 首先将数据中所有的中文提取出来\n",
    "            wordList = \"/\".join(jieba.cut(Data)) # 使用jieba进行切词，并且把每个词语使用/分隔开\n",
    "            listOfTokens = wordList.split(\"/\") # \n",
    "            x= [tok for tok in listOfTokens if (tok not in stopWords and len(tok) >= 2)] # 注意python3 写成一行的if 语句需要用括号\n",
    "            x=list(set(x)) # 使用词集\n",
    "            '''\n",
    "            词集和词袋的区别：\n",
    "            词集里面只考虑每个词是否出现\n",
    "            词袋同时也会考虑每个词出现的次数\n",
    "            \n",
    "            '''\n",
    "            results.append(' '.join(x))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试数据集\n",
    "\n",
    "X = ['我们都是好朋友',\n",
    "    '这里有一个人叫做坏人','这是一条新闻','新华社说这么多我也不知道是为什么']\n",
    "y = [1,1,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainpath = ''\n",
    "# testpath = ''\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(__name__=='__main__'):\n",
    "    nbclassifier=NBclassifier(is_fit=False)\n",
    "    \n",
    "    dataList,labelList=nbclassifier.loadtrainData(trainPath, 50000)\n",
    "    testData,testLabel=nbclassifier.loadtestData(testPath, 50000)\n",
    "    # 训练并预测分类正确性\n",
    "    nbclassifier.trainNB(dataList, labelList)\n",
    "    predictList=nbclassifier.predictNB(testData)\n",
    "    probs=nbclassifier.preprob(testData)\n",
    "    predictList2,predictList_topk=nbclassifier.get_nbest(probs,labelList,0.3)\n",
    "    nbclassifier.calAccuracy(predictList, testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否从数据中学习类别的先验概率 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长度是40000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dumping model to file cache /var/folders/b7/t4jynbgx15l4n6vhy95bl47m0000gn/T/jieba.cache\n",
      "Loading model cost 1.651 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "长度是2512\n",
      "[-4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915 -4.39444915\n",
      " -4.39444915 -4.39444915 -4.39444915]\n",
      "测试数据集个数为 2512\n",
      "准确率为：0.25039808917197454\n"
     ]
    }
   ],
   "source": [
    "# 使用不同的训练集和测试集\n",
    "if(__name__=='__main__'):\n",
    "    \n",
    "    nbclassifier=NBclassifier(is_fit=False)\n",
    "    trainPath='data/train_data/train_t1'  # id \\t title \\t content \\t type \\t single label \\t multi label \n",
    "    testPath='data/train_data/test_t1'  # id \\t title \\t content \\t type \\t single label \\t multi label \n",
    "    dataList,labelList=nbclassifier.loadtrainData(trainPath, 50000)\n",
    "    testData,testLabel=nbclassifier.loadtestData(testPath, 50000)\n",
    "    # 训练并预测分类正确性\n",
    "    nbclassifier.trainNB(dataList, labelList)\n",
    "    predictList=nbclassifier.predictNB(testData)\n",
    "    probs=nbclassifier.preprob(testData)\n",
    "    predictList2,predictList_topk=nbclassifier.get_nbest(probs,labelList,0.3)\n",
    "    nbclassifier.calAccuracy(predictList, testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测结果，并加上label。这里应该直接变成格式\n",
    "testPath=\"/Users/lijunyi/美团/头条/data/sep_data/data_to_process/t3_process\"\n",
    "predata,testLabel=nbclassifier.loadtestData(testPath, 50000)\n",
    "predictList=nbclassifier.predictNB(predata)\n",
    "probs=nbclassifier.preprob(predata)\n",
    "predictList2,predictList_topk=nbclassifier.get_nbest(probs,labelList,0.3)\n",
    "import sys\n",
    "count = 0\n",
    "output=open('/Users/lijunyi/Desktop/李露/t3_res_prob','w')\n",
    "with open('/Users/lijunyi/美团/头条/data/sep_data/data_to_process/t3_process') as train:\n",
    "    for line in train:\n",
    "        line = line.strip()\n",
    "        output.write(line+'\\t'+str(predictList2[count])+'\\n')\n",
    "        count+=1\n",
    "    print(count, file=sys.stderr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision：0.2976672450356661\n",
      "recall：0.3180881747012773\n",
      "f1-measure: 0.3075390897320984\n"
     ]
    }
   ],
   "source": [
    "# 从数据中学习先验概率\n",
    "nbclassifier.cal_precision(predictList2, testLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26010\n"
     ]
    }
   ],
   "source": [
    "probs=nbclassifier.preprob(predata)\n",
    "predictList2,predictList_topk=nbclassifier.get_nbest(probs,labelList,0.3)\n",
    "import sys\n",
    "count = 0\n",
    "output=open('/Users/lijunyi/Desktop/李露/t1_res_prob','w')\n",
    "with open('/Users/lijunyi/美团/头条/data/sep_data/data_to_process/t1_process') as train:\n",
    "    for line in train:\n",
    "        line = line.strip()\n",
    "        output.write(line+'\\t'+str(predictList2[count])+'\\n')\n",
    "        count+=1\n",
    "    print(count, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26010\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "count = 0\n",
    "output=open('/Users/lijunyi/Desktop/李露/t1_res_prob','w')\n",
    "with open('/Users/lijunyi/美团/头条/data/sep_data/data_to_process/t1_process') as train:\n",
    "    for line in train:\n",
    "        line = line.strip()\n",
    "        output.write(line+'\\t'+str(predictList2[count])+'\\n')\n",
    "        count+=1\n",
    "    print(count, file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否从数据中学习类别的先验概率 False\n",
      "长度是7000\n",
      "长度是4512\n",
      "[-4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887 -4.71849887\n",
      " -4.71849887 -4.71849887 -4.71849887 -4.71849887]\n",
      "测试数据集个数为 4512\n",
      "准确率为：0.00022163120567375886\n"
     ]
    }
   ],
   "source": [
    "# 使用不同的训练集和测试集\n",
    "if(__name__=='__main__'):\n",
    "    nbclassifier=NBclassifier(is_fit=False)\n",
    "    #数据集地址及生成的训练集与测试集地址\n",
    "#     dataPath=u'../../datasets/answer'\n",
    "#     trainPath='data/trainData.txt'\n",
    "#     testPath='data/testData.txt'\n",
    "    trainPath='data/train_data/t3_train'\n",
    "    testPath='data/train_data/t1_test'\n",
    "#     dataList,labelList=nbclassifier.loadTexts(dataPath)\n",
    "#     dataList = data_nosource.head(54000)['content'].values.tolist()\n",
    "#     dataList=nbclassifier.loadData(data_nosource.head(5)['content'].values.tolist()) # 这个使用了数据读取的函数 问题就在这里\n",
    "#     labelList=data_nosource.head(54000)['class'].values.tolist()\n",
    "    # 使用全量数据进行随机划分产生训练数据以及测试数据\n",
    "#     nbclassifier.generateSample(data, label,trainPath,testPath)\n",
    "    # 载入训练集与测试集 \n",
    "    dataList,labelList=nbclassifier.loadtrainData(trainPath, 10000)\n",
    "    testData,testLabel=nbclassifier.loadtestData(testPath, 10000)\n",
    "    # 训练并预测分类正确性\n",
    "    nbclassifier.trainNB(dataList, labelList)\n",
    "    predictList=nbclassifier.predictNB(testData)\n",
    "    probs=nbclassifier.preprob(testData)\n",
    "    predictList2,predictList_topk=nbclassifier.get_nbest(probs,labelList,0.3)\n",
    "    nbclassifier.calAccuracy(predictList, testLabel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
