{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道词向量的应用很多，一般来说词向量是作为一个词语的向量空间表示。或者说是语义表示，所以我们可以使用这个词向量来代表很多语义信息。然后基于此可以做很多新鲜的应用。做文章很多时候都是在这个基础上进行的。我们可以对词做不同的切词，或者做不同的处理，进而训练出不同的词向量。而对词向量的应用，也是可以有不同的方式方法，而这些方式方法就是我们可以一起考虑的很多东西。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/b7/t4jynbgx15l4n6vhy95bl47m0000gn/T/jieba.cache\n",
      "Loading model cost 0.869 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#!/bin/env python3\n",
    "#-*- encoding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import codecs\n",
    "import re\n",
    "from collections import Counter\n",
    "import jieba\n",
    "# from conf import nlp_cfg\n",
    "# from log import g_log_inst as logger\n",
    "\n",
    "\n",
    "class NLPUtil(object):\n",
    "    _valid_token_len = 5\n",
    "\n",
    "    _wordseg_pattern_cfg = [\n",
    "        re.compile(r'{.*?}', re.U),\n",
    "    ]\n",
    "\n",
    "    # _emoji_pattern_cfg = re.compile(r'[\\U0001f600-\\U0001f9ef]', re.U)\n",
    "    _emoji_pattern_cfg = re.compile(u'('\n",
    "        u'\\ud83c[\\udf00-\\udfff]|'\n",
    "        u'\\ud83d[\\udc00-\\ude4f\\ude80-\\udeff]|'\n",
    "        u'[\\u2600-\\u2B55])+', flags=re.UNICODE)\n",
    "\n",
    "    _replace_pattern_cfg = {\n",
    "        'float_t': re.compile('\\d+\\.\\d+'),\n",
    "        'phone_t': re.compile(r'1[0-9\\*]{10}|\\d{3}[-\\s]\\d{4}[-\\s]\\d{4}|\\+861[0-9]{10}|[0-9]{3}-[0-9]{3}-[0-9]{4}|[0-9]{4}-[0-9]{7,8}|[8|6][0-9]{7}'),\n",
    "        'email_t': re.compile(r'[^@|\\s]+@[^@]+\\.[^@|\\s]+'),\n",
    "    }\n",
    "\n",
    "    replace_patterns = [\n",
    "        # ('date_t' , re.compile(ur'\\d{2,4}-\\d{1,2}-\\d{1,2}|\\d{2,4}\\.\\d{1,2}\\.\\d{1,2}|\\d{1,2}\\.\\d{1,2}|[0-9一二三四五六七八九十]{1,2}月[0-9一二三四五六七八九十]{1,2}[日号]?|[0-9一二三四五六七八九十]{1,2}月[份]?|[0-9一二三四五六七八九十]{1,2}[日号]')),\n",
    "        # ('time_t' , re.compile(ur'\\d{1,2} :\\d{1,2}:\\d{1,2}|\\d{1,2}:\\d{1,2}|\\d{1,2}点\\d{1,2}分?|\\d{1,2}点半?')),\n",
    "        # ('url_t'  , re.compile(r'(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]|www\\.[.*]\\.[cn|com]')),\n",
    "        ('', re.compile(r'\\[.*\\]'))\n",
    "    ]\n",
    "\n",
    "    _illegal_char_set = set([])\n",
    "\n",
    "    # init jieba\n",
    "    jieba.initialize()\n",
    "    # load user-define dictionary\n",
    "    # if os.path.exists(nlp_cfg['jieba_dict_fpath']):\n",
    "    #     with codecs.open(nlp_cfg['jieba_dict_fpath'], 'r', 'utf8') as in_f:\n",
    "    #         _ = map(lambda x: nlp_cfg['g_ud_words_cfg'].add(x.strip('\\n')), in_f.readlines())\n",
    "    # for w in nlp_cfg['g_ud_words_cfg']:\n",
    "    #     jieba.add_word(w, freq = 1000000)\n",
    "    # # print 'load user-define jieba dict success!'\n",
    "    # # load stopwords\n",
    "    # if os.path.exists(nlp_cfg['stopword_fpath']):\n",
    "    #     with codecs.open(nlp_cfg['stopword_fpath'], 'r', 'utf-8') as in_f:\n",
    "    #         words = map(lambda w: w.strip('\\n'), in_f.readlines())\n",
    "    #         _ = map(lambda x: nlp_cfg['g_stop_words_cfg'].add(x), words)\n",
    "        # print 'load stopwords success!'\n",
    "\n",
    "    @classmethod\n",
    "    def remove_illegal_gbk_char(cls, text_unicode):\n",
    "        try:\n",
    "            text_unicode.encode('gbk')\n",
    "            return text_unicode\n",
    "        except UnicodeEncodeError as e:\n",
    "            illegal_ch = e.object[e.start : e.end]\n",
    "            illegal_set = cls._illegal_char_set\n",
    "            illegal_set.add(illegal_ch)\n",
    "            # try to replace directly\n",
    "            for ch in illegal_set:\n",
    "                text_unicode = text_unicode.replace(ch, '')\n",
    "            # remove recursively\n",
    "            return cls.remove_illegal_gbk_char(text_unicode)\n",
    "\n",
    "    @classmethod\n",
    "    def remove_emoji_char(cls, text_unicode):\n",
    "        res = cls._emoji_pattern_cfg.sub('', text_unicode)\n",
    "        return res\n",
    "\n",
    "\n",
    "    # @classmethod\n",
    "    # def conv_fenc_u8_to_gbk(cls, in_fpath, out_fpath):\n",
    "    #     try:\n",
    "    #         with codecs.open(in_fpath, 'r', 'utf-8') as rfd, \\\n",
    "    #             codecs.open(out_fpath, 'w', 'gbk') as wfd:\n",
    "    #             # read utf8, write gbk\n",
    "    #             for line in rfd:\n",
    "    #                 line = cls.remove_illegal_gbk_char(line)\n",
    "    #                 wfd.write(line)\n",
    "    #     except Exception as e:\n",
    "    #         logger.get().warn('errmsg=%s' % (e))\n",
    "\n",
    "    @classmethod\n",
    "    def tokenize_via_jieba(cls, text, normalize=True, filter_stop_word = False):\n",
    "        # remove emoji\n",
    "        text = cls.remove_emoji_char(text)\n",
    "        # normalize text\n",
    "        if normalize:\n",
    "            text = cls._normalize_text(text)\n",
    "            # print 'after normalize_text:',text\n",
    "        tokens = jieba.lcut(text.lower())\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    # @classmethod\n",
    "    # def stat_token_freq(cls, in_fpath, out_fpath):\n",
    "    #     stop_words = nlp_cfg['g_stop_words_cfg']\n",
    "    #     try:\n",
    "    #         word_counter = Counter()\n",
    "    #         with codecs.open(in_fpath, 'r', 'utf-8') as rfd:\n",
    "    #             for line in rfd:\n",
    "    #                 raw_str, word_seg = line.strip('\\n').split('\\t')\n",
    "    #                 tokens = word_seg.split()\n",
    "    #                 tokens = filter(lambda x: x not in stop_words, tokens) \n",
    "    #                 tokens = map(cls._normalize_token, tokens)\n",
    "    #                 for t in tokens:\n",
    "    #                     if ('{[' not in t) and len(t) <= cls._valid_token_len:\n",
    "    #                         word_counter[t] += 1\n",
    "    #                     else:\n",
    "    #                         logger.get().warn('invalid token, token=%s' % (t))\n",
    "    #                         # tokenize via jieba \n",
    "    #                         for n_t in jieba.cut(t):\n",
    "    #                             word_counter[n_t] += 1\n",
    "    #                             logger.get().debug('jieba cut, token=%s' % (n_t))\n",
    "    #         # dump word_counter\n",
    "    #         sorted_words = sorted(word_counter.keys(),\n",
    "    #             key = lambda k: word_counter[k], reverse = True)\n",
    "    #         with codecs.open(out_fpath, 'w', 'utf-8') as wfd:\n",
    "    #             for word in sorted_words:\n",
    "    #                 tmp = '%s\\t%s\\n' % (word, word_counter[word]) \n",
    "    #                 wfd.write(tmp)\n",
    "    #     except Exception as e:\n",
    "    #         logger.get().warn('errmsg=%s' % (e))\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def _normalize_token(cls, token):\n",
    "        token = token.lower()\n",
    "        try:\n",
    "            # 11 usually means phone number\n",
    "            if len(token) != 11 and token.isdigit():\n",
    "                token = 'int_t'\n",
    "            for k, v in cls._replace_pattern_cfg.items():\n",
    "                if v.match(token):\n",
    "                    token = k\n",
    "                    break\n",
    "            if '{[' not in token:\n",
    "                return token\n",
    "            for item in cls._wordseg_pattern_cfg:\n",
    "                token = item.sub('', token)\n",
    "            return token\n",
    "        except Exception as e:\n",
    "            logger.get().warn('token=%s, errmsg=%s' % (token, e))\n",
    "            return token\n",
    "\n",
    "    @classmethod\n",
    "    def _normalize_text(cls, text): \n",
    "        the_patterns = []\n",
    "        for i,(name, pattern) in enumerate(cls.replace_patterns):\n",
    "            if pattern.search(text): \n",
    "                the_patterns.append((pattern, name))\n",
    "        if not the_patterns:\n",
    "            return text\n",
    "        else:\n",
    "            replaced_str = text\n",
    "            for pattern, name in the_patterns:\n",
    "                replaced_str = re.sub(pattern, name, replaced_str)\n",
    "            return replaced_str\n",
    "\n",
    "# if '__main__' == __name__:\n",
    "#     logger.start('./log/test.log', __name__, 'DEBUG')\n",
    "    '''\n",
    "        test tokenize\n",
    "    '''\n",
    "    # print '|'.join(NLPUtil.tokenize_via_jieba(u'1月1日到6月30日'))\n",
    "    # print '|'.join(NLPUtil.tokenize_via_jieba(u'我就问首单红包怎么用不了'))\n",
    "    # print '|'.join(NLPUtil.tokenize_via_jieba(u'应该是9月9号入住'))\n",
    "    # print NLPUtil._normalize_text('133****5454')\n",
    "    # print NLPUtil._normalize_text(u'1月1日到6月30日')\n",
    "    \n",
    "    '''\n",
    "        normalize text\n",
    "    '''\n",
    "    # with codecs.open('../data/question/qunar.question.dat', 'r', 'utf8') as in_f, \\\n",
    "    #     codecs.open('../data/question/new_qunar.question.dat', 'w', 'utf8') as out_f:\n",
    "    #     for line in in_f:\n",
    "    #         line = line.strip('\\n')\n",
    "    #         line = NLPUtil._normalize_text(line)\n",
    "    #         out_f.write(line + '\\n')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
